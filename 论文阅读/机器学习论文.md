###  Attention is all you need
#### 术语及背景知识
* 相关知识点：
	* 神经网络基础 [5分钟搞懂RNN，3D动画深入浅出](https://www.bilibili.com/video/BV1z5411f7Bm/?spm_id_from=333.337.search-card.all.click&vd_source=8636d68797fa4651942df4dc09db7987)
	* 前馈神经网络：[前馈神经网络 & 反馈神经网络](https://blog.csdn.net/hxxjxw/article/details/107524450)
		最简单的神经网络，每个神经元至于前一层的神经元相连，接受前一层输出给下一层
	* 逻辑回归 [【机器学习】逻辑回归十分钟学会](https://www.bilibili.com/video/BV1PJ411676g/?spm_id_from=333.337.search-card.all.click&vd_source=8636d68797fa4651942df4dc09db7987)
		[[逻辑回归]]
	* Attention [Attention 注意力机制](https://www.bilibili.com/video/BV1xS4y1k7tn/?spm_id_from=333.788&vd_source=8636d68797fa4651942df4dc09db7987)[Attention注意力机制博客](https://blog.51cto.com/u_15794627/5682736#:~:text=%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C%E5%8F%AF%E8%A7%86%E4%B8%BA,%EF%BC%8C%E8%BF%91%E4%BC%BC%E4%BA%8E%E7%AD%9B%E9%80%89%E5%99%A8%E3%80%82)
		**Attention含义即是权重**![[Pasted image 20221220223648.png]]
		直接使用两个RNN的Encoder-Decoder模型将输入压缩成统一相同长度编码c的时，c能蕴含的信息有限，会导致翻译精度下降：![[Pasted image 20221220211944.png]]
		Attention通过每个时间输入不同的c来解决这个问题![[Pasted image 20221220212030.png]]
		![[Pasted image 20221220212223.png]]
		以Ct的视角看，ati就是t时刻不同输入的注意力，可以通过神经网络训练得到最好的Attention矩阵
		Attention引入打破了只能使用Encoder形成单一向量的限制，使每一时刻模型都能动态地看到全局信息，将注意力集中到对当前单词翻译最终要的信息上
		* 随着GPU与并行运算发展，RNN难以并行运算，效率太低，简化了RNN中的时间相关序列（Attention本身已经对不同位置输入进行打分）
		* 将这个去除时间序列而引入注意力权重的模型放入前馈神经网络重复几次可以得到更好的效果![[Pasted image 20221220222459.png]]
	* Transformer理解：
		[【Transformer模型】](https://www.bilibili.com/video/BV1MY41137AK/?spm_id_from=333.788&vd_source=8636d68797fa4651942df4dc09db7987)![[Pasted image 20221220224509.png]]
		数据流动过程：
		1. 单词向量化，嵌入位置信息，变成统一长度，成为一个个输入的定长向量
		2. 输入单词向量化后的矩阵到Self-Attention和前馈神经网络，结果发送到下一个编码器
	* Transformer中**位置编码详解** [Transformer模型的位置编码](https://zhuanlan.zhihu.com/p/106644634)
		 [Transformer Architecture: The Positional Encoding - Amirhossein Kazemnejad's Blog](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)![[Pasted image 20221220231106.png]]![[Pasted image 20221220230944.png]]
	* SoftMax与标准化
		[一文详解Softmax函数](https://zhuanlan.zhihu.com/p/105722023)
		[数据预处理之特征缩放: 归一化(Normalization)&标准化(Standardization)](https://www.bilibili.com/video/BV1sA411Y7Ww/?spm_id_from=333.337.search-card.all.click&vd_source=8636d68797fa4651942df4dc09db7987)
		目的：不至于使一个特征的数值过于巨大而掩盖其他特征。去量纲
		**归一化：**![[Pasted image 20221221005707.png]]特点：对异常值特别敏感（如果样本中某个特征出现了特别小或者特别大的值，则会影响归一化的整体结果，出现非常小或非常大的值）
		**标准化：**
		假设input的所有特征满足正态分布，则希望把它转换成服从标准正态分布的空间中![[Pasted image 20221221010347.png]]
		特点：对异常值不太敏感，缩放到标准正态分布上
	* 激活函数
		[如何理解激活函数？（附常用激活函数）](https://zhuanlan.zhihu.com/p/364620596)
	* Byte-Pair encodding
		[理解NLP最重要的编码方式 — Byte Pair Encoding (BPE)](https://zhuanlan.zhihu.com/p/424631681)


#### 论文阅读
##### 模型解读
* [Transformer从零详细解读](https://www.bilibili.com/video/BV1Di4y1c7Zm/?spm_id_from=333.337.search-card.all.click&vd_source=8636d68797fa4651942df4dc09db7987)
###### 1. 全局概括
![[Pasted image 20221220173838.png|300]]
Transformer的结构整体上是由Encoder和Decoder构成，层数可以自定义，每一个Encoder结构相同，每一个Decoder结构相同，其中各层的参数单独训练，Encoder和Decoder结构不同。
###### 2.Encoder层细节
	![[Pasted image 20221220174212.png]]
1. 输入部分
	1. Embedding：将字化为一个个等长的向量
	2. 位置编码：
		RNN的所有的TimeStep共享一套参数![[Pasted image 20221220183525.png]]
		而对于Transfomer，处理时有并行化所有单词一起处理，增快了速度忽略了单词间序列关系，这时需要位置编码![[Pasted image 20221220183917.png]]偶数位置使用sin，基数位置使用cos，位置编码和word的embedding向量结合产生输入![[Pasted image 20221220184101.png]]
	3. 注意力机制![[Pasted image 20221220184957.png]]softmax归一化函数，将一个实向量压缩为同维度的另一个实向量，保持各个维度一定的比例并且所有元素范围在（0，1）之间，所有元素和为1![[Pasted image 20221220190018.png]]
		Q:query 需要注意的内容向量，Query，点乘能反映向量的相似度
		K:key 对输入内容的各区域划分向量组成的矩阵
		V:value 各区域划分向量的某种值向量![[Pasted image 20221220190426.png]]![[Pasted image 20221220190654.png]]
		Q,K,V的获取：通过input经过带位置编码的embedding然后与各个W矩阵相乘（训练获得的权重矩阵）![[Pasted image 20221220204734.png]]
		**实际代码使用矩阵计算方便并行**![[Pasted image 20221220204911.png]]
		* **多头注意力机制**：使用多套参数。合并多个输出作为多头注意力机制的输出![[Pasted image 20221220205103.png]]![[Pasted image 20221220205208.png]]
	4. 残差和LayNorm
		![[Pasted image 20221220205306.png]]
		关于残差：![[Pasted image 20221220205433.png]]
		将输入和输出对位相加得到结果![[Pasted image 20221220205539.png]]
		梯度消失（梯度趋近于0）一般是连乘导致，此时残差的引入可以缓解梯度消失的出现，可以做更深层的网络
	5. Batch和Layer Normalization
		* 在NLP中Bath Normalization一般效果不好，一般用Layer Normalization（一个batch的样本相同位置往往没有联系（没有同样的语义信息），而单个样本中有联系）![[Pasted image 20221221001557.png]]
		* 关于Bath Normalization 在做BN时针对一个Batch中所有样本的同一维度做BN![[Pasted image 20221221001208.png]]
		* 关于Layer Normalization
###### Decoder
	![[Pasted image 20221221002845.png]]
		mask:解码端预测时是一个个解码，看不到当前单词之后的信息，所以训练时也需要把这个信息抹掉
		交互层：Encoder最后的输出和每一个Decoder都进行交互![[Pasted image 20221221003103.png]]
		
	


##### 论文精读
[Transformer论文逐段精读【论文精读】](https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.788&vd_source=8636d68797fa4651942df4dc09db7987)
###### 1.摘要和结论
  本文提出了Transformer神经网络架构，它是只基于注意力，完全抛弃了回环和卷积的序列转导模型。它成功地应用于机器翻译并且效率可以高于原有的基于回环和卷积层的架构。
  
###### 2.导言
  2017年的时序模型中最常用的是RNN，其中有两个主流模型，其一是语言模型，其二是编码器-解码器模型。它的缺点是难以并行，且早期的时序信息在后期可能会丢掉。本文提出的Transformer完全依赖于注意力机制提高了并行性
  
###### 3.相关工作
  卷积神经网络对较长序列难以建模，但可以有多输出通道（识别不一样的模式），而Transformer没有这个问题，且可以通过多头注意力机制模拟多通道效果
  
###### 4.模型架构
* 架构：
	编码器接受n个词向量，解码器输出m个词向量。**解码器的输出是一个个生成的，编码器可以一次性看到整个句子但解码时只能一个个生成**。这是一个自回归的模型，过去时刻的输出也会作为当前时刻的输入，编码器的输出会作为解码器的输入。
* 具体模块实现：
	* 编码器：
		![[Pasted image 20221221194258.png]]一个编码器被称作一个layer，本模型重复N = 六个layer，每个layer中有两个sub-layer，分别是多头注意力和前馈神经网络，每个子层都使用了残差连接和layer normalization，每个子层输出公式如下![[Pasted image 20221221194652.png]]
		为了保证输出和输入维度一致方便做残差连接，设置输出维度为$d_{dimension} =$ 512
	* 解码器：
		和编码器一样是由N = 6层构成，其中有三个字子层，第一个子层是带掩码的注意力，保证在t时刻不会看到之后的输入，保证训练和预测时行为是一致的。
	* 注意力层
		注意力函数本质上是将query和key-value对映射成输出的函数，所有的query，key，value和output都是向量。
		本质上**output是value的加权和**，输出的维度和value的维度是一致的
		权重是由query和key的相似度算来的，不同注意力机制有不同算法，随着query不一样，权重不同，导致输出会不一样
		* Scaled Dot-Product Attention
			本文中query和key有相同的维度$d_k$，对每一个query和key做点积作为相似度，算出来之后除以$\sqrt{d_k}$，再放进softmax（对每一行做softmax，**一行的意义为一个query和各个key的相似度**），则对于每一个query和其对应的所有的key，可以得到一个元素非负加起来等于1长度为d的权重向量。再将权重作用到value上就得到输出了。
			实际计算中使用矩阵运算方便并行：![[Pasted image 20221221202245.png]]![[Pasted image 20221221215306.png]]
			 为什么要除以$\sqrt{d_k}$: 当$d_k$比较大时，两个向量比较长，点积的值可能会比较大或比较小（出现特例），就导致softmax值出来会有权重更加接近1，其他权重更加接近0，权重分布方差更大，以至于梯度比较小
		* Multi-Head Attention
			与其做单个注意力函数，不如把整个query，key，value投影到低维，投影h次，再做h此注意力函数，然后把每一个函数的输出并在一起，再投影回来得到最终输出![[Pasted image 20221221215707.png]]
			目的：单头的注意力Query和Key相似函数是固定的，多头注意力是为了识别不一样的模式。投影W可以通过学习得到，给h次机会希望模型学到不一样的投影方法，使得再投影空间里能够去匹配不同模式它需要的一些相似函数。可以类比卷积神经网络多个通道。![[Pasted image 20221221220114.png]]
			通过可以学习的$W^Q W^K W^V$将Q K V投影到低维再做之前的注意力函数。
			因为有残差连接，输入和输出相同，则每一个头输出为$d/h$，当h为8时是64维
		* 模型中注意力的使用
			* 编码器中的注意力
				假设编码器长度为n，则编码器输入是n个长度为d的向量![[Pasted image 20221221194258.png]]
				此处注意力层有三个输入，分别是Query，Key和Value，注意到三者从同一根线引出，即注意力层输入的Query Key Value都是从同一输入变换过来的，所谓**自注意力机制**
				对每个Query，会有一个长度为d的输出，每一个输出都是Value的一个加权和，权重来自于Query和Key。每个token的Query肯定和自己对应的Key的相似度最大，权重最高。
				在不考虑多头和投影时，输出就是输入的加权和，权重来自于本身跟其他输入向量的相似度。
			* 解码器中的注意力
				mask注意力机制需要将当前词向量之后的词向量的权重设为零![[Pasted image 20221221225206.png]]
				**解码器第二层的注意力不再是自注意力了，Key和Value来自编码器的输出，Query来自解码器第一层attendtion的输入**
				第二层注意力所作的事情即是有效地把编码器的输出根据所想要的东西提取出来。根据解码器输入的不一样，Query不一样，Query和编码器输出的Key计算出权重不一样。
	* 前馈神经网络
		实质上是一个全连接前馈神经网络，也即多层感知机，把**同样一个MLP**对每一个position（每一个词）作用一次![[Pasted image 20221222115944.png]]
		是单隐藏层MLP，输入时把维度为d = 512的输入升维到2048，输出再降维到512做残差连接。
		作用：进行语义空间的转换
		* 整个流程的理解
			![[Pasted image 20221222120335.png]]
			attention起的作用是把整个输入序列的信息抓取出来做一次汇聚，得到每个词向量在整个序列中感兴趣的信息。在做投影映射到更想要的语义空间时，每一个向量都包含了序列信息，每个MLP只需要对每个点独立做即可
		* 和RNN的对比![[Pasted image 20221222120858.png]]两者都使用MLP进行语义空间的转换，不同的是**如何传递序列的信息**，RNN是把上一时刻的信息输出传入下一时刻做输入，而Transformer是通过attention层去全局拿到序列信息再用MLP做于一转换
	* Embedding与Softmax
		* embedding：对任何一个词通过训练学习一个长度为d的向量
	* Positional Encoding
		为什么需要：**Attention本身是不会有时序信息的**，输出是value的加权和，权重是query和key相似度，是和序列信息是无关的（把序列打乱attention出来的结果是一样的，顺序会随着变但值不会变）
		做法是在输入时加入时序信息，即Positional Encoding，用一个和词向量相同长度的向量表示位置信息（0，1，2....），具体每个元素使用不同周期的正弦余弦函数表示
		相当于对数字做embedding,然后再和词的embedding加起来作为Encoder输入
		

###### 5.为什么用自注意力
![[Pasted image 20221222123516.png]]
	计算复杂度: n * d 的query和key矩阵转置相乘
	Sequential Operation:矩阵乘法并行度较高 
	Path Length:一个点的信息要跳到另一个点的信息要多少步, attention中是关注全局信息加权计算,只需要一步
	自注意力机制在上面三个维度性能都比较好

###### 6.训练
* 训练数据:
	英语德语翻译文中使用standard WMT 2014 English-German dataset, 使用byte-pair encoding(提取词根),词汇库有37000个token,是英语德语共享的,意味着编码器和解码器的embedding可以相同,可以使模型更简单.英语法语翻译使用WMT 2014 English-French dataset
* 训练器与学习率:
	使用Adam optimizer
	![[Pasted image 20221222143041.png]]
* 正则化:
	Residual Dropout:
		对每一个子层输出上,进入残差连接和LayerNorm之前使用Dropout
		在embedding和positional encoding进入编码器解码器前也使用Dropout
	Label Smoothing

#### 论文源码与复现
[Transformer代码(源码Pytorch版本)从零解读](https://www.bilibili.com/video/BV1dR4y1E7aL/?spm_id_from=333.788.recommend_more_video.0&vd_source=8636d68797fa4651942df4dc09db7987)




#### 困惑记录
##### 输入长度问题
* **Transformer为什么可以输入任意长度的句子？每一个Token的attention是如何生成的？**
帖子中猛猿的回答[Transformer是如何处理可变长度数据的](https://www.zhihu.com/question/445895638)
对于不同的token，使用的是**相同的训练好的Q K V矩阵**，得到对应的Q K V向量![[Pasted image 20221220234154.png]]
**使用当前token的Q乘以所有token的K**
![[Pasted image 20221220234901.png]]
多头注意力机制：
	使用不同权重矩阵进行多次计算![[Pasted image 20221220235607.png]]为了消除Q K V初始值的影响，最后求加权平均![[Pasted image 20221220235646.png]]
##### 输出长度问题
* **Transformer如何决定输出序列的长度，Decoder的输入是什么**
	[Transformer的Decoder的输入输出都是什么](https://www.zhihu.com/question/337886108)





