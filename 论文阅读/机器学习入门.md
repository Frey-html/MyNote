####  相关资料
* 神经网络入门
	[从零开始神经网络第一期](https://www.bilibili.com/video/BV1m4411x7KU/?spm_id_from=333.788.recommend_more_video.2&vd_source=8636d68797fa4651942df4dc09db7987)
	[图解，卷积神经网络（CNN可视化）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1x44y1P7s2/?spm_id_from=333.788&vd_source=8636d68797fa4651942df4dc09db7987)

* 基础知识：
	* 神经网络层结构![[Pasted image 20221219171404.png]]
	* 关于卷积神经网络：
		* 卷积：提取图像特征，每一个卷积核对应一个图像特征，图像中的要素与卷积核越相似，则卷积后所得矩阵内数值越大![[Pasted image 20221220171644.png]]
		* 池化：卷积后提取出的图像特征矩阵可能也非常大，也可能提取到许多弱的特征，需要降维，这就是池化![[Pasted image 20221220171814.png]]方法：对一块区域内的值取等效值。可以是最大值或平均值，显著降低矩阵大小。可以加快运算速度，也能很大程度避免过拟合，提高泛化能力![[Pasted image 20221220172004.png]]池化原理：图像中的相邻像素倾向于有相似的值，通常卷积层输出相邻也有相似的值，意味着卷积层大多数信息都是冗余的。池化解决了信息冗余。
		* 激活层：主要对池化层输出进行非线性映射，CNN中一般为ReLU函数![[Pasted image 20221220172327.png]]相较于Sigmoid，ReLU函数计算量较小。对于深层网络，Sigmoid在反向传播时容易出现梯度消失。
		* 卷积，池化，激活只是三种操作，如何组合选取因工作而异![[Pasted image 20221220172646.png]]
	* 循环神经网络![[Pasted image 20221220174837.png]]网络一层的输入不仅与上一层的输出有关，还与本层前一次的输出有关，为神经网络加入时序和上下文的知识。![[Pasted image 20221220175459.png]]由于时序上的层级结构，使RNN在输入输出关系上具备更大的灵活性，如1 to N（看图说话）,N to 1（语句情感判断或按描述输出图片），N to N（等长，生成诗歌，代码等），N to M（Encoder-Decoder模型或Seq2Seq模型）![[Pasted image 20221220175918.png]]先将输入编码成上下文向量再根据它输出，用于翻译等
	* LSTM:Long Short Term Memory
		RNN记忆能力有限，不能无限延伸，一般超过十步效果就不好了![[Pasted image 20221220210145.png]]网络结构：新增链条记录长期记忆
		根据上一次的该层输出st-1和本次输入xt对长期记忆进行删除和增加![[Pasted image 20221220210733.png]]删除过程通过sigmoid过滤重要特征，减少无关信息（forget gate），增加过程通过tanh对信息进行梳理和归纳，以此得到长期记忆链ct和短期记忆链st，相互更新


#### 动手练习