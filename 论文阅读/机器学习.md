###  Attention is all you need
#### 术语及背景知识
* 相关知识点：
	* 神经网络基础 [5分钟搞懂RNN，3D动画深入浅出](https://www.bilibili.com/video/BV1z5411f7Bm/?spm_id_from=333.337.search-card.all.click&vd_source=8636d68797fa4651942df4dc09db7987)
	* 前馈神经网络：[前馈神经网络 & 反馈神经网络](https://blog.csdn.net/hxxjxw/article/details/107524450)
		最简单的神经网络，每个神经元至于前一层的神经元相连，接受前一层输出给下一层
	* 逻辑回归 [【机器学习】逻辑回归十分钟学会](https://www.bilibili.com/video/BV1PJ411676g/?spm_id_from=333.337.search-card.all.click&vd_source=8636d68797fa4651942df4dc09db7987)
		[[逻辑回归]]
	* Attention [Attention 注意力机制](https://www.bilibili.com/video/BV1xS4y1k7tn/?spm_id_from=333.788&vd_source=8636d68797fa4651942df4dc09db7987)[Attention注意力机制博客](https://blog.51cto.com/u_15794627/5682736#:~:text=%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C%E5%8F%AF%E8%A7%86%E4%B8%BA,%EF%BC%8C%E8%BF%91%E4%BC%BC%E4%BA%8E%E7%AD%9B%E9%80%89%E5%99%A8%E3%80%82)
		**Attention含义即是权重**![[Pasted image 20221220223648.png]]
		直接使用两个RNN的Encoder-Decoder模型将输入压缩成统一相同长度编码c的时，c能蕴含的信息有限，会导致翻译精度下降：![[Pasted image 20221220211944.png]]
		Attention通过每个时间输入不同的c来解决这个问题![[Pasted image 20221220212030.png]]
		![[Pasted image 20221220212223.png]]
		以Ct的视角看，ati就是t时刻不同输入的注意力，可以通过神经网络训练得到最好的Attention矩阵
		Attention引入打破了只能使用Encoder形成单一向量的限制，使每一时刻模型都能动态地看到全局信息，将注意力集中到对当前单词翻译最终要的信息上
		* 随着GPU与并行运算发展，RNN难以并行运算，效率太低，简化了RNN中的时间相关序列（Attention本身已经对不同位置输入进行打分）
		* 将这个去除时间序列而引入注意力权重的模型放入前馈神经网络重复几次可以得到更好的效果![[Pasted image 20221220222459.png]]
	* Transformer理解：
		[【Transformer模型】](https://www.bilibili.com/video/BV1MY41137AK/?spm_id_from=333.788&vd_source=8636d68797fa4651942df4dc09db7987)![[Pasted image 20221220224509.png]]
		数据流动过程：
		1. 单词向量化，嵌入位置信息，变成统一长度，成为一个个输入的定长向量
		2. 输入单词向量化后的矩阵到Self-Attention和前馈神经网络，结果发送到下一个编码器
	* Transformer中**位置编码详解** [Transformer模型的位置编码](https://zhuanlan.zhihu.com/p/106644634)
		 [Transformer Architecture: The Positional Encoding - Amirhossein Kazemnejad's Blog](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)![[Pasted image 20221220231106.png]]![[Pasted image 20221220230944.png]]
	* SoftMax与标准化
		[一文详解Softmax函数](https://zhuanlan.zhihu.com/p/105722023)
		[数据预处理之特征缩放: 归一化(Normalization)&标准化(Standardization)](https://www.bilibili.com/video/BV1sA411Y7Ww/?spm_id_from=333.337.search-card.all.click&vd_source=8636d68797fa4651942df4dc09db7987)
		目的：不至于使一个特征的数值过于巨大而掩盖其他特征。去量纲
		**归一化：**![[Pasted image 20221221005707.png]]特点：对异常值特别敏感（如果样本中某个特征出现了特别小或者特别大的值，则会影响归一化的整体结果，出现非常小或非常大的值）
		**标准化：**
		假设input的所有特征满足正态分布，则希望把它转换成服从标准正态分布的空间中![[Pasted image 20221221010347.png]]
		特点：对异常值不太敏感，缩放到标准正态分布上


#### 论文阅读
##### 模型解读
* [Transformer从零详细解读](https://www.bilibili.com/video/BV1Di4y1c7Zm/?spm_id_from=333.337.search-card.all.click&vd_source=8636d68797fa4651942df4dc09db7987)
###### 1. 全局概括
![[Pasted image 20221220173838.png|300]]
Transformer的结构整体上是由Encoder和Decoder构成，层数可以自定义，每一个Encoder结构相同，每一个Decoder结构相同，其中各层的参数单独训练，Encoder和Decoder结构不同。
###### 2.Encoder层细节
	![[Pasted image 20221220174212.png]]
1. 输入部分
	1. Embedding：将字化为一个个等长的向量
	2. 位置编码：
		RNN的所有的TimeStep共享一套参数![[Pasted image 20221220183525.png]]
		而对于Transfomer，处理时有并行化所有单词一起处理，增快了速度忽略了单词间序列关系，这时需要位置编码![[Pasted image 20221220183917.png]]偶数位置使用sin，基数位置使用cos，位置编码和word的embedding向量结合产生输入![[Pasted image 20221220184101.png]]
	3. 注意力机制![[Pasted image 20221220184957.png]]softmax归一化函数，将一个实向量压缩为同维度的另一个实向量，保持各个维度一定的比例并且所有元素范围在（0，1）之间，所有元素和为1![[Pasted image 20221220190018.png]]
		Q:query 需要注意的内容向量，Query，点乘能反映向量的相似度
		K:key 对输入内容的各区域划分向量组成的矩阵
		V:value 各区域划分向量的某种值向量![[Pasted image 20221220190426.png]]![[Pasted image 20221220190654.png]]
		Q,K,V的获取：通过input经过带位置编码的embedding然后与各个W矩阵相乘（训练获得的权重矩阵）![[Pasted image 20221220204734.png]]
		**实际代码使用矩阵计算方便并行**![[Pasted image 20221220204911.png]]
		* **多头注意力机制**：使用多套参数。合并多个输出作为多头注意力机制的输出![[Pasted image 20221220205103.png]]![[Pasted image 20221220205208.png]]
	4. 残差和LayNorm
		![[Pasted image 20221220205306.png]]
		关于残差：![[Pasted image 20221220205433.png]]
		将输入和输出对位相加得到结果![[Pasted image 20221220205539.png]]
		梯度消失（梯度趋近于0）一般是连乘导致，此时残差的引入可以缓解梯度消失的出现，可以做更深层的网络
	5. Batch和Layer Normalization
		* 在NLP中Bath Normalization一般效果不好，一般用Layer Normalization（一个batch的样本相同位置往往没有联系（没有同样的语义信息），而单个样本中有联系）![[Pasted image 20221221001557.png]]
		* 关于Bath Normalization 在做BN时针对一个Batch中所有样本的同一维度做BN![[Pasted image 20221221001208.png]]
		* 关于Layer Normalization
###### Decoder
	![[Pasted image 20221221002845.png]]
		mask:解码端预测时是一个个解码，看不到当前单词之后的信息，所以训练时也需要把这个信息抹掉
		交互层：Encoder最后的输出和每一个Decoder都进行交互![[Pasted image 20221221003103.png]]
		
	


##### 论文精读
[Transformer论文逐段精读【论文精读】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.788&vd_source=8636d68797fa4651942df4dc09db7987)
1. 摘要和结论
	本文提出了Transformer神经网络架构，它是只基于注意力，完全抛弃了回环和卷积的序列转导模型。它成功地应用于机器翻译并且效率可以高于原有的基于回环和卷积层的架构。
2. 导言
	2017年的时序模型中最常用的是RNN，其中有两个主流模型，其一是语言模型，其二是编码器-解码器模型。它的缺点是难以并行，且早期的时序信息在后期可能会丢掉。
	本文提出的Transformer完全依赖于注意力机制提高了并行性
3. 相关工作
	 卷积神经网络对较长序列难以建模，但可以有多输出通道（识别不一样的模式），而Transformer没有这个问题，且可以通过多头注意力机制模拟多通道效果
4. 模型架构


#### 困惑记录
##### Transformer为什么可以输入任意长度的句子？每一个Token的attention是如何生成的？
帖子中猛猿的回答[Transformer是如何处理可变长度数据的](https://www.zhihu.com/question/445895638)
对于不同的token，使用的是**相同的训练好的Q K V矩阵**，得到对应的Q K V向量![[Pasted image 20221220234154.png]]
**使用当前token的Q乘以所有token的K**
![[Pasted image 20221220234901.png]]
多头注意力机制：
	使用不同权重矩阵进行多次计算![[Pasted image 20221220235607.png]]为了消除Q K V初始值的影响，最后求加权平均![[Pasted image 20221220235646.png]]




#### 小练习 
* 神经网络入门
	[从零开始神经网络第一期](https://www.bilibili.com/video/BV1m4411x7KU/?spm_id_from=333.788.recommend_more_video.2&vd_source=8636d68797fa4651942df4dc09db7987)
	[图解，卷积神经网络（CNN可视化）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1x44y1P7s2/?spm_id_from=333.788&vd_source=8636d68797fa4651942df4dc09db7987)

* 基础知识：
	* 神经网络层结构![[Pasted image 20221219171404.png]]
	* 关于卷积神经网络：
		* 卷积：提取图像特征，每一个卷积核对应一个图像特征，图像中的要素与卷积核越相似，则卷积后所得矩阵内数值越大![[Pasted image 20221220171644.png]]
		* 池化：卷积后提取出的图像特征矩阵可能也非常大，也可能提取到许多弱的特征，需要降维，这就是池化![[Pasted image 20221220171814.png]]方法：对一块区域内的值取等效值。可以是最大值或平均值，显著降低矩阵大小。可以加快运算速度，也能很大程度避免过拟合，提高泛化能力![[Pasted image 20221220172004.png]]池化原理：图像中的相邻像素倾向于有相似的值，通常卷积层输出相邻也有相似的值，意味着卷积层大多数信息都是冗余的。池化解决了信息冗余。
		* 激活层：主要对池化层输出进行非线性映射，CNN中一般为ReLU函数![[Pasted image 20221220172327.png]]相较于Sigmoid，ReLU函数计算量较小。对于深层网络，Sigmoid在反向传播时容易出现梯度消失。
		* 卷积，池化，激活只是三种操作，如何组合选取因工作而异![[Pasted image 20221220172646.png]]
	* 循环神经网络![[Pasted image 20221220174837.png]]网络一层的输入不仅与上一层的输出有关，还与本层前一次的输出有关，为神经网络加入时序和上下文的知识。![[Pasted image 20221220175459.png]]由于时序上的层级结构，使RNN在输入输出关系上具备更大的灵活性，如1 to N（看图说话）,N to 1（语句情感判断或按描述输出图片），N to N（等长，生成诗歌，代码等），N to M（Encoder-Decoder模型或Seq2Seq模型）![[Pasted image 20221220175918.png]]先将输入编码成上下文向量再根据它输出，用于翻译等
	* LSTM:Long Short Term Memory
		RNN记忆能力有限，不能无限延伸，一般超过十步效果就不好了![[Pasted image 20221220210145.png]]网络结构：新增链条记录长期记忆
		根据上一次的该层输出st-1和本次输入xt对长期记忆进行删除和增加![[Pasted image 20221220210733.png]]删除过程通过sigmoid过滤重要特征，减少无关信息（forget gate），增加过程通过tanh对信息进行梳理和归纳，以此得到长期记忆链ct和短期记忆链st，相互更新
