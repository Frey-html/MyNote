#### Attention is all you need
##### 论文精读
[Transformer论文逐段精读【论文精读】](https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.788&vd_source=8636d68797fa4651942df4dc09db7987)
###### 1.摘要和结论
  本文提出了Transformer神经网络架构，它是只基于注意力，完全抛弃了回环和卷积的序列转导模型。它成功地应用于机器翻译并且效率可以高于原有的基于回环和卷积层的架构。
  
###### 2.导言
  2017年的时序模型中最常用的是RNN，其中有两个主流模型，其一是语言模型，其二是编码器-解码器模型。它的缺点是难以并行，且早期的时序信息在后期可能会丢掉。本文提出的Transformer完全依赖于注意力机制提高了并行性
  
###### 3.相关工作
  卷积神经网络对较长序列难以建模，但可以有多输出通道（识别不一样的模式），而Transformer没有这个问题，且可以通过多头注意力机制模拟多通道效果
  
###### 4.模型架构
* 架构：
	编码器接受n个词向量，解码器输出m个词向量。**解码器的输出是一个个生成的，编码器可以一次性看到整个句子但解码时只能一个个生成**。这是一个自回归的模型，过去时刻的输出也会作为当前时刻的输入，编码器的输出会作为解码器的输入。
* 具体模块实现：
	* 编码器：
		![[Pasted image 20221221194258.png]]一个编码器被称作一个layer，本模型重复N = 六个layer，每个layer中有两个sub-layer，分别是多头注意力和前馈神经网络，每个子层都使用了残差连接和layer normalization，每个子层输出公式如下![[Pasted image 20221221194652.png]]
		为了保证输出和输入维度一致方便做残差连接，设置输出维度为$d_{dimension} =$ 512
	* 解码器：
		和编码器一样是由N = 6层构成，其中有三个字子层，第一个子层是带掩码的注意力，保证在t时刻不会看到之后的输入，保证训练和预测时行为是一致的。
	* 注意力层
		注意力函数本质上是将query和key-value对映射成输出的函数，所有的query，key，value和output都是向量。
		本质上**output是value的加权和**，输出的维度和value的维度是一致的
		权重是由query和key的相似度算来的，不同注意力机制有不同算法，随着query不一样，权重不同，导致输出会不一样
		* Scaled Dot-Product Attention
			本文中query和key有相同的维度$d_k$，对每一个query和key做点积作为相似度，算出来之后除以$\sqrt{d_k}$，再放进softmax（对每一行做softmax，**一行的意义为一个query和各个key的相似度**），则对于每一个query和其对应的所有的key，可以得到一个元素非负加起来等于1长度为d的权重向量。再将权重作用到value上就得到输出了。
			实际计算中使用矩阵运算方便并行：![[Pasted image 20221221202245.png|300]]![[Pasted image 20221221215306.png]]
			 为什么要除以$\sqrt{d_k}$: 当$d_k$比较大时，两个向量比较长，点积的值可能会比较大或比较小（出现特例），就导致softmax值出来会有权重更加接近1，其他权重更加接近0，权重分布方差更大，以至于梯度比较小
		* Multi-Head Attention
			与其做单个注意力函数，不如把整个query，key，value投影到低维，投影h次，再做h此注意力函数，然后把每一个函数的输出并在一起，再投影回来得到最终输出![[Pasted image 20221221215707.png]]
			目的：单头的注意力Query和Key相似函数是固定的，多头注意力是为了识别不一样的模式。投影W可以通过学习得到，给h次机会希望模型学到不一样的投影方法，使得再投影空间里能够去匹配不同模式它需要的一些相似函数。可以类比卷积神经网络多个通道。![[Pasted image 20221221220114.png]]
			通过可以学习的$W^Q W^K W^V$将Q K V投影到低维再做之前的注意力函数。
			因为有残差连接，输入和输出相同，则每一个头输出为$d/h$，当h为8时是64维
		* 模型中注意力的使用
			* 编码器中的注意力
				假设编码器长度为n，则编码器输入是n个长度为d的向量![[Pasted image 20221221194258.png]]
				此处注意力层有三个输入，分别是Query，Key和Value，注意到三者从同一根线引出，即注意力层输入的Query Key Value都是从同一输入变换过来的，所谓**自注意力机制**
				对每个Query，会有一个长度为d的输出，每一个输出都是Value的一个加权和，权重来自于Query和Key。每个token的Query肯定和自己对应的Key的相似度最大，权重最高。
				在不考虑多头和投影时，输出就是输入的加权和，权重来自于本身跟其他输入向量的相似度。
			* 解码器中的注意力
				mask注意力机制需要将当前词向量之后的词向量的权重设为零![[Pasted image 20221221225206.png|300]]
				**解码器第二层的注意力不再是自注意力了，Key和Value来自编码器的输出，Query来自解码器第一层attendtion的输入**
				第二层注意力所作的事情即是有效地把编码器的输出根据所想要的东西提取出来。根据解码器输入的不一样，Query不一样，Query和编码器输出的Key计算出权重不一样。
	* 前馈神经网络
		实质上是一个全连接前馈神经网络，也即多层感知机，把**同样一个MLP**对每一个position（每一个词）作用一次![[Pasted image 20221222115944.png]]
		是单隐藏层MLP，输入时把维度为d = 512的输入升维到2048，输出再降维到512做残差连接。
		作用：进行语义空间的转换
		* 整个流程的理解
			![[Pasted image 20221222120335.png]]
			attention起的作用是把整个输入序列的信息抓取出来做一次汇聚，得到每个词向量在整个序列中感兴趣的信息。在做投影映射到更想要的语义空间时，每一个向量都包含了序列信息，每个MLP只需要对每个点独立做即可
		* 和RNN的对比![[Pasted image 20221222120858.png]]两者都使用MLP进行语义空间的转换，不同的是**如何传递序列的信息**，RNN是把上一时刻的信息输出传入下一时刻做输入，而Transformer是通过attention层去全局拿到序列信息再用MLP做于一转换
	* Embedding与Softmax
		* embedding：对任何一个词通过训练学习一个长度为d的向量
	* Positional Encoding
		为什么需要：**Attention本身是不会有时序信息的**，输出是value的加权和，权重是query和key相似度，是和序列信息是无关的（把序列打乱attention出来的结果是一样的，顺序会随着变但值不会变）
		做法是在输入时加入时序信息，即Positional Encoding，用一个和词向量相同长度的向量表示位置信息（0，1，2....），具体每个元素使用不同周期的正弦余弦函数表示
		相当于对数字做embedding,然后再和词的embedding加起来作为Encoder输入
		

###### 5.为什么用自注意力
![[Pasted image 20221222123516.png]]
	计算复杂度: n * d 的query和key矩阵转置相乘
	Sequential Operation:矩阵乘法并行度较高 
	Path Length:一个点的信息要跳到另一个点的信息要多少步, attention中是关注全局信息加权计算,只需要一步
	自注意力机制在上面三个维度性能都比较好

###### 6.训练
* 训练数据:
	英语德语翻译文中使用standard WMT 2014 English-German dataset, 使用byte-pair encoding(提取词根),词汇库有37000个token,是英语德语共享的,意味着编码器和解码器的embedding可以相同,可以使模型更简单.英语法语翻译使用WMT 2014 English-French dataset
* 训练器与学习率:
	使用Adam optimizer
	![[Pasted image 20221222143041.png]]
* 正则化:
	Residual Dropout:
		对每一个子层输出上,进入残差连接和LayerNorm之前使用Dropout
		在embedding和positional encoding进入编码器解码器前也使用Dropout
	Label Smoothing