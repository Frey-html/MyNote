展翼飞 3190102196
### Adversarial Examples: Attacks and Defenses for Deep Learning
#### Abstract and Introduction
本文主题为对抗样本：对于深度学习的攻击和防御，由浅入深地分析了对抗样本的背景，分类，生成，应用，防御与挑战。在摘要中，作者主要介绍了在深度学习被广泛运用的情景下，对抗样本出现了。对抗样本是一种被精心设计的输入样本，人眼难以察觉却可以欺骗深度神经网络模型。本文主要总结了对抗样本的 生成方法并提出了一种分类方法用以解决对抗样本带来的威胁。
深度学习不断发展，在大数据和硬件加速的影响下，不断应用在现实世界的各种重要程序系统中，也引起了人们对其安全问题的关注，其中对抗样本是对于深度学习神经网络安全的一大威胁。了解和防御对抗样本，既可以解决一部分安全问题，又可以在深度学习“黑匣子”技术的背景下深入了解神经网络的语义，找到有问题的决策边界，有助于提高神经网络的鲁棒性和性能并提高可解释性。
在研究和总结了生成对抗样本的方法、对抗样本的应用和相应的对策的同时，作者定义了对抗样本的**威胁模型**：
* 对抗样本只能在测试/部署阶段攻击
* 专注于针对使用深度神经网络构建的模型的攻击，因为它们实现了出色的性能（并且对抗深度神经网络的对抗性例子在传统的机器学习模型中被证明是有效的）
* 对抗样本只旨在损害完整性。完整性由性能指标（例如准确性、F1 分数、AUC）呈现。
并做出了分类攻击方法，使用此分类法对最近的对抗样本攻击研究进行比较，概述了基于三个主要问题的对抗样本的主要挑战和潜在未来的研究方向等贡献。

#### Background
本节简要介绍了与对抗样本相关的基本深度学习技术和方法并回顾了传统ML时代的对抗性例子，比较了传统ML中的对抗性例子与DL中的对抗性例子之间的差异。
**深度学习简介**：深度学习是一种机器学习方法，它使计算机无需显式编程即可从经验和知识中学习，并从原始数据中提取有用的模式。CNN 在隐藏层上部署卷积操作以共享权重并减少参数的数量，CNN可以从网格状输入数据中提取局部信息。RNN 是用于处理可变长度的顺序输入数据的神经网络。RNN 在每个时间步产生输出，设计了具有可控门的长短期记忆 (LSTM) 和门控循环单元 (GRU)，以避免 RNN 在长期依赖中的消失/爆炸梯度。
**机器学习中的对抗样本和对策**：
Barreno等人将针对机器学习系统的攻击分为三个维度：1）影响：攻击是否可以毒化训练数据； 2）安全违规：对抗样本是否属于假阳性或假阴性； 3）特异性：攻击针对特定实例或广泛类别。传统机器学习中的对抗样本需要特征提取的知识，而深度学习通常只需要原始数据输入。例如，图像分类任务中的对抗样本：对抗样本图像是具有小扰动的原始干净图像，通常人类几乎无法识别。然而，这种扰动误导了图像分类器。用户将获得错误图像标签的响应。

#### Taxonomy of Adversarial Examples
**对抗样本的分类**：沿三个维度进行分类：威胁模型、扰动和基准
**威胁模型**：威胁模型可分解为四个方面：对抗性伪造（假阳性与假阴性）、攻击者的知识（白盒攻击与黑盒攻击）、对抗性特异性（有针对性攻击和无针对性攻击）和攻击频率（一次性攻击和迭代性攻击）。
**扰动**：小扰动是对抗样本的基本前提。作者分析了扰动的三个方面：扰动范围、扰动限制和扰动测量。
扰动范围：个体攻击为每个干净的输入产生不同的扰动。通用攻击只为整个数据集创建一个通用扰动，这种扰动可以应用于所有干净的输入数据。
扰动限制：可分为优化扰动与约束扰动。优化扰动旨对抗样本攻击达到一定效果的情况下最小化扰动使识别扰动更加困难。约束扰动则旨在一定扰动范围内最大化对抗样本的攻击效果。
扰动测量：可用对抗样本与干净样本相差的p范数或者PASS（Psychometric perceptual adversarial similarity）标准。
**基准**：数据集和受害者模型的不一致性给评估对抗性攻击和测量深度学习模型的鲁棒性带来了障碍。大型和高质量的数据集，复杂和高性能的深度学习模型通常使对手/防御者难以攻击/防御。

#### Methods for generating Adversarial Examples
本节说明了几种用于生成对抗样本的代表性方法:L-BFGS Attack,Fast Gradient Sign Method,Basic Iterative Method (BIM) and Iterative Least-Likely Class Method (ILLC),Jacobian-based Saliency Map Attack (JSMA),DeepFool,CPPN EA Fool,Zeroth Order Optimization (ZOO),Universal Perturbation,One Pixel Attack,Feature Adversary,Hot/Cold,Natural GAN,Model-based Ensembling Attack,Ground-Truth Attack, 介绍这些对抗样本攻击方法的同时展示了对抗样本攻击是如何改进的。

#### Applications for Adversarial Examples
本节主要关注三个问题:对抗样本在新任务中应用的场景是什么?如何在新任务中生成对抗样本?是提出新的方法，还是将问题转化为图像分类任务，用上述方法解决?之后分析了对抗样本在以下新场景中的应用与改进：
**强化学习**：由于强化学习的计算量较大，之前的研究都采用了快速一次性攻击的方法。其中，Huang等人应用了快速梯度符号法（FGSM）来攻击深度强化学习网络和算法，对白盒攻击和黑盒攻击都进行了成功的攻击。
**生成式模型**：Kos等人和Tabacof等提出了生成式模型的对抗样本。自编码器的攻击者可以在编码器的输入中注入扰动，并在解码后生成目标类。损失函数J可以是交叉熵、VAE损失函数和原始潜在向量z与修改后编码向量x′之间的距离(“潜在攻击”）。
**面部识别**：基于深度神经网络的人脸识别系统(FRS)和人脸检测系统因其高性能而在商业产品中得到了广泛的应用。被引论文设计了一种带有38层的11个区块和一个三元损失函数的深度神经网络FRS。然后，他们利用三元损失函数设计了一个softmax损失函数，并使用L-BFGS攻击生成对抗样本。
**物体识别**：被引论文提出了一种名为密集对抗生成（Dense Adversary Generation，DAG）的通用算法，用于生成目标检测和语义分割的对抗样本，使预测（检测/分割）不正确（非定向攻击）。
**语义分割**：图像分割任务可以视为对每个像素进行图像分类的任务。由于每个扰动至少负责一个像素的分割，这使得分割的扰动空间要比图像分类的空间小得多。一些研究生成了针对语义图像分割任务的对抗样本，一些攻击是针对非定向分割的，另一些攻击是针对特定目标进行的，试图通过欺骗深度学习模型将其误导为背景类。
**自然语言处理**：在阅读理解的场景中，任务的模型是过度稳定的而不是过度敏感的，这意味着深度学习模型无法分辨段落中微妙但关键的差异。被引论文提出了两种方法来生成对抗样本：1）添加与问题类似但不与正确答案相矛盾的语法句子（AddSent）；2）添加带有任意英语单词的句（AddAny）。
**恶意软件检测**：最近的研究生成了对抗性恶意软件样本以逃避基于深度学习的恶意软件检测。这些方法有使用JSMA方法攻击Android恶意软件检测模型、 修改PDF以逃避两种PDF恶意软件分类器等。

#### Countermeasures for Adversarial Examples
针对对抗样本的对策有两种防御策略:1)反应性:在构建深度神经网络后检测对抗样例;2)主动性:在对手生成对抗样本之前，使深度神经网络更加鲁棒。在本节中，作者介绍了三种被动对策(对抗检测、输入重建和网络验证)和三种主动对策(网络蒸馏、对抗训练和分类器鲁棒化)。
**网络蒸馏**：网络蒸馏最初是为了通过将知识从大型网络转移到小型网络来减小深度神经网络的大小，而在对抗样本防御中第一个DNN产生的类的概率被用作训练第二个DNN的输入。类的概率提取了从第一个DNN学到的知识。在Softmax函数中引入温度T，用于控制知识蒸馏的级别。在当T很大时，softmax的输出将模糊（当T → ∞时，所有类的概率 → 1/m）。当T很小时，只有一个类接近1，而其他类接近0。
**对抗(重)训练**：训练神经网络时使用对抗样本是增强其鲁棒性的一种方法，这样做可以提高深度神经网络的鲁棒性，对抗性训练不仅可以提供正则化，还可以提高精度。
**对抗检测**：一些研究使用基于深度神经网络的二元分类器作为检测器，将输入数据分类为合法输入或对抗样本。一些方法还利用贝叶斯神经网络估计输入数据的不确定性，并基于不确定性估计来区分对抗样本和干净输入数据。
**输入重构**：通过重构，对抗样本可以转化为干净数据。在转化后，对抗样本不会影响深度学习模型的预测结果。
**分类器鲁棒化**：Bradshaw等人利用贝叶斯分类器构建更加强鲁棒的神经网络，通过高斯过程（GP）和RBF核提供不确定性估计。
**网络验证**：是防御对抗样本的一种有前途的解决方案，因为它可以检测到新的未见攻击。网络验证检查神经网络的属性：输入是否违反或满足该属性。

##### CHALLENGES AND DISCUSSIONS
本节讨论了以下问题:为什么对抗样本会转移?如何阻止可转让性?为什么有些防御措施有效，有些则无效?如何衡量进攻和防御的强度?如何评估深度神经网络对可见/未见对抗样本的鲁棒性?
**迁移性**：迁移性对于黑盒攻击至关重要，其中受害深度学习模型和训练数据集不可访问，攻击者可以训练一个替代的神经网络模型，然后针对替代模型生成对抗样本，然后由于迁移性，受害模型将对这些对抗性示例易受攻击。从防御者的角度来看，如果我们阻碍对抗样本的迁移性，我们就可以抵御所有需要访问模型并要求传递性的白盒攻击者。作者从易到难定义了对抗样本的三个级别的传递性：1) 在使用不同数据训练的相同神经网络架构之间的传递；2) 在为相同任务训练的不同神经网络架构之间的传递；3) 在不同任务的深度神经网络之间的传递。
**对抗样本存在性**：对抗样本的存在原因仍然存疑。对抗性示例是深度神经网络的固有属性吗？许多假设已被提出来解释其存在。数据不完整：一种假设是对抗样本在测试数据集中是低概率、低测试覆盖率的边缘案例；模型能力：对抗性样本不仅是深度神经网络的现象，也适用于所有分类器。部分研究者认为对抗样本的出现是模型在高维流形中过于线性化的结果；模型缺乏鲁棒性：部分学者认为深度神经网络的决策边界本质上是不正确的，不能检测语义对象。
**鲁棒性评估**：最初为防止现有攻击而提出的防御方法后来被证明对某些新攻击易受攻击，反之亦然。一些防御方法表明它们可以抵御特定攻击，但稍加修改后却失败了。因此，对深度神经网络的鲁棒性进行评估是必要的。