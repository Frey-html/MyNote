学号：3190102196  姓名：展翼飞
***
#### 问题重述

![Image](https://imgbed.momodel.cn/20200914145238.png)

如上图所示，左上角的红色椭圆既是起点也是机器人的初始位置，右下角的绿色方块是出口。  
游戏规则为：从起点开始，通过错综复杂的迷宫，到达目标点(出口)。
- 在任一位置可执行动作包括：向上走 `'u'`、向右走 `'r'`、向下走 `'d'`、向左走 `'l'`。
- 执行不同的动作后，根据不同的情况会获得不同的奖励，具体而言，有以下几种情况。
    - 撞墙
    - 走到出口
    - 其余情况
在本实验中，我们需要分别使用基础搜索算法和 Deep QLearning 算法，完成机器人自动走迷宫。

#### 设计思想
##### 基础搜索算法
本次实现深度优先搜索算法，使用深度优先搜索完成机器人走迷宫的任务。将已经给出的广度优先搜索算法修改为深度优先搜索算法，只需要做如下调整：
1. 使用栈代替队列：DFS 使用栈结构，而 BFS 使用队列结构。可以用列表的 append 和 pop方法来模拟栈的行为。
2. 修改入栈和出栈的顺序：在 DFS 中，我们将新节点压入栈顶，并从栈顶弹出节点处理。

##### DQN 算法
DQN 算法使用神经网络近似 QLearning 中的 Q值函数，它包含以下步骤
1. **初始化**：
    - 初始化参数，包括 epsilon, gamma, learning_rate, 等。
    - 创建 Q 网络和目标网络，以及优化器。
    - 创建一个经验回放池。
2. **选择动作**：
    - 使用 ε-贪心策略选择动作。
    - 如果随机数小于 epsilon，随机选择动作，否则选择 Q 网络估计的最优动作。
3. **经验回放**：
    - 从经验回放池中随机抽取一个小批量进行训练。
    - 计算 Q 目标值，基于目标网络的估计。
    - 计算 Q 期望值，基于 Q 网络的估计。
    - 计算损失并进行反向传播和优化。
4. **训练更新**：
    - 每次训练步时，执行一次动作，获取当前状态、动作和奖励，更新经验池。
    - 执行一次经验回放，更新 Q 网络参数。
    - 随时间衰减 epsilon 值。
5. **测试更新**：
    - 在测试模式下选择动作并返回。

#### 代码内容
##### 基础搜索算法
![[Pasted image 20240529102827.png]]

##### DQN 算法
1. **初始化方法 __init__**:
    - 设置了各种超参数，包括 epsilon, gamma, learning_rate等。
    - 初始化 Q 网络和目标网络，并使用 Adam优化器。
    - 调用 update_target_network方法初始化目标网络参数。
2. **软更新方法 update_target_network**:
    - 使用软更新策略逐步更新目标网络的参数，以确保训练的稳定性。
3. **记忆经验方法 remember**:
    - 将当前经验存储到经验回放池中，方便后续训练使用。
4. **选择动作方法 act**:
    - 采用ε-贪心策略，根据当前状态选择动作。
    - 当随机数小于 epsilon 时，随机选择动作；否则，选择 Q 网络估计的最优动作。
5. **经验回放方法 repla**:
    - 从经验回放池中随机抽取一个小批量进行训练。
    - 计算 Q 目标值和 Q 期望值，进行损失计算和反向传播。
6. **训练更新方法 train_update**:
    - 获取当前状态并选择动作，执行动作后存储经验。
    - 调用 replay 方法进行经验回放和训练，逐步减小探索概率 epsilon。
7. **测试更新方法 test_update**:
    - 在测试模式下选择动作并返回。
![[Pasted image 20240531172358.png]]
![[Pasted image 20240531172414.png]]
![[Pasted image 20240531172432.png]]
![[Pasted image 20240531172446.png]]

#### 实验结果
![[Pasted image 20240531172323.png]]